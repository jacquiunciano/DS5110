{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972d117d-4640-42cd-86f7-64c99ed9fed9",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "In this part, you will implement a simple Spark application. We have provided some sample data collected at this link (using wget). Download the file to your home directory of vm1.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e1abaa-1816-41e2-ae75-38eeef42f49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/spark-3.3.1-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark as fs\n",
    "fs.init('/home/ubuntu/spark-3.3.1-bin-hadoop3')\n",
    "fs.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4813edd0-f149-4435-806e-e19e43bece31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/26 11:24:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"DS5110\")\n",
    "            .master(\"spark://172.31.75.157:7077\")\n",
    "            .config(\"spark.executor.memory\", \"1024M\")\n",
    "            .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8769f24d-16d1-4efa-9927-de43096eaf06",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: hdfs://172.31.75.157:9000/export.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://172.31.75.157:9000/export.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/sql/readwriter.py:535\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: hdfs://172.31.75.157:9000/export.csv"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/26 11:26:42 ERROR TaskSchedulerImpl: Lost executor 0 on 172.31.75.157: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "24/02/26 11:26:43 WARN StandaloneAppClient$ClientEndpoint: Connection to 172.31.75.157:7077 failed; waiting for master to reconnect...\n",
      "24/02/26 11:26:43 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n",
      "24/02/26 11:26:43 WARN StandaloneAppClient$ClientEndpoint: Connection to 172.31.75.157:7077 failed; waiting for master to reconnect...\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"hdfs://172.31.75.157:9000/export.csv\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507510c2-8baa-43b3-b7ae-f689e664d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"battery_level\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1076d3-0bdb-4452-9860-8de65b2105ef",
   "metadata": {},
   "source": [
    "You then need to sort the data firstly by the country code alphabetically (the third column ccr2) then by the timestamp (the last column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0747a85-6b7f-4edb-bfdc-d4a5100e0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.orderBy(\"cca2\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e5fd2-71aa-49f3-a892-3d1a0c4b001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeab1d9-ff8e-4a50-9ef7-4b361385ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.75.157:9000/new_df_hdfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e38c7c-6bd1-4db1-a68c-f28e51606a7d",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "In this part, you will implement the PageRank algorithm (§2.1) (or the Wikipedia version), which is an algorithm used by search engines like Google to evaluate the quality of links to a webpage. The algorithm can be summarized as follows:\n",
    "\n",
    "1. Set initial rank of each page to be 1.\n",
    "2. On each iteration, each page p contributes to its outgoing neighbors a value of rank(p)/(# of outgoing neighbors of p).\n",
    "3. Update each page’s rank to be 0.15 + 0.85 * (sum of contributions).\n",
    "4. Go to next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad4c6cc-0e6a-4032-97de-ac54f56d554c",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "Write a PySpark application that implements the PageRank algorithm. Your PageRank application should output the following two results: 1) print the first 50 rows with the highest ranks; 2) save the computed results as a Spark DF to HDFS as an HDFS csv file."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3b0710f-2e5b-49e4-b7b1-c6e9d2edc5df",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col, collect_list, explode, lit\n",
    "data = spark.read.text(\"hdfs://172.31.75.157:9000/web-BerkStan.txt\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "359e1fd2-beb7-4ea7-990c-11acdde3d9f3",
   "metadata": {},
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5772a20b-9635-412a-9665-1cc0ad2eadc4",
   "metadata": {},
   "source": [
    "# extract the page and its outgoing links from the data given\n",
    "links = data.select(\n",
    "    col(\"value\").substr(1, 10).alias(\"page\"),\n",
    "    col(\"value\").substr(12, 10).alias(\"outlink\")\n",
    ").groupBy(\"page\").agg(collect_list(\"outlink\").alias(\"outlinks\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55187073-a938-4e12-a2fa-d60066bb905d",
   "metadata": {},
   "source": [
    "links.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19d0dc70-a89d-48df-b42e-0560a4de1a7e",
   "metadata": {},
   "source": [
    "# make df ==> Set initial rank of each page to be 1.\n",
    "df = links.withColumn(\"rank\", lit(1.0))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82aa1fe6-bed7-4203-a25c-32eb39f3513a",
   "metadata": {},
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b450268-c50a-428f-bb79-d01151296bb9",
   "metadata": {},
   "source": [
    "for i in range(3):\n",
    "    # Join the DataFrame with itself to calculate contributions\n",
    "    # each page p contributes to its outgoing neighbors \n",
    "    # a value of rank(p)/(# of outgoing neighbors of p)\n",
    "    contributions = df.join(\n",
    "        df.selectExpr(\"explode(outlinks) as outlink\", \"page as page2\", \"rank as rank2\"),\n",
    "        col(\"outlink\") == col(\"page2\"),\n",
    "        \"left\"\n",
    "    ).selectExpr(\"page\", \"outlink\", \"ifnull(rank2 / size(outlinks), 0) as contribution\")\n",
    "\n",
    "    # Update each page’s rank to be 0.15 + 0.85 * (sum of contributions).\n",
    "    df = df.join(\n",
    "        contributions.groupBy(\"outlink\").sum(\"contribution\").withColumnRenamed(\"sum(contribution)\", \"cont_sum\"),\n",
    "        col(\"page\") == col(\"outlink\"),\n",
    "        \"left\"\n",
    "    ).selectExpr(\"page\", \"outlinks\", \"ifnull(0.15 + 0.85 * cont_sum, 0) as rank\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d603d5f-f59f-4fa8-a056-920bf507a0bf",
   "metadata": {},
   "source": [
    "# first 50 rows with the highest ranks\n",
    "top_50 = df.orderBy(col(\"rank\").desc()).limit(50)\n",
    "top_50.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75537453-3a30-4fce-8473-654e63395f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import Iterable, Tuple\n",
    "from pyspark.resultiterable import ResultIterable\n",
    "\n",
    "# from skeleton code:\n",
    "# Helper function to calculate URL contributions to the rank of other URLs\n",
    "def calculateRankContrib(urls: ResultIterable[str], rank: float) -> Iterable[Tuple[str, float]]:\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "# # from skeleton code:\n",
    "# Helper function to parse a urls string into a tuple of URLs\n",
    "def parseNeighborURLs(urls: str) -> Tuple[str, str]:\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac975aa0-1f07-46fc-8ea5-309747984df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "linesRDD = spark.sparkContext.textFile(\"hdfs://172.31.75.157:9000/web-BerkStan.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75776ff5-7f9b-4b83-b8f7-62a802ca9f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linksRDD = linesRDD.map(lambda urls: parseNeighborURLs(urls)).distinct().groupByKey()\n",
    "ranksRDD = linksRDD.map(lambda url_neighbors: (url_neighbors[0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b30e4-bab4-4ce8-a9c7-5479e2ef77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    # Join the links RDD with the ranks RDD and calculate contributions\n",
    "    contributions = linksRDD.join(ranksRDD).flatMap(lambda x: calculateRankContrib(x[1][0], x[1][1]))\n",
    "    # Update ranks based on contributions\n",
    "    ranksRDD = contributions.reduceByKey(lambda x, y: x + y).mapValues(lambda x: 0.15 + 0.85 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff84da-ec99-49e1-8bdd-dd5898477413",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksDF = ranksRDD.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d63725-7ca1-4a6d-9b09-ad9651820796",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksDF.write.format(\"csv\").save(\"hdfs://172.31.75.157:9000/ranksDF_pt1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e286243-974f-4d30-a65e-87e5248b2526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 50 URLs by rank\n",
    "top_50 = ranksRDD.takeOrdered(50, key=lambda x: -x[1])\n",
    "# Print the top 50\n",
    "for url, rank in top_50:\n",
    "    print(f\"URL: {url}, Rank: {rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c307e58-538e-42a7-baff-55f37d65f3e8",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "In order to achieve high parallelism, Spark will split the data into smaller chunks called partitions, which are distributed across different nodes in the cluster. Partitions can be changed in several ways. For example, any shuffle operation on an RDD (e.g., join()) will result in a change in partitions (customizable via user’s configuration). In addition, one can also decide how to partition data when creating/configuring RDDs (hint: e.g., you can use the function partitionBy()). For this task, add appropriate custom RDD partitioning and see what changes. For the computed result: your PageRank application should print the first 50 rows with the highest ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211ae44-21b2-4731-b2bc-483727d94bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as before but use partitionBy\n",
    "linksRDD = linesRDD.map(lambda urls: parseNeighborURLs(urls))\\\n",
    ".distinct().groupByKey().partitionBy(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a35ec3-4cc3-465e-a29a-fdf7ded3cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a ranks RDD\n",
    "ranksRDD = linksRDD.map(lambda url_neighbors: (url_neighbors[0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0610ce-441a-48b7-b988-a7cd26c5cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and updates URL ranks continuously using PageRank algorithm\n",
    "for i in range(3):\n",
    "    # Join the links RDD with the ranks RDD and calculate contributions\n",
    "    contributions = linksRDD.join(ranksRDD).flatMap(lambda x: calculateRankContrib(x[1][0], x[1][1]))\n",
    "    # Update ranks based on contributions\n",
    "    ranksRDD = contributions.reduceByKey(lambda x, y: x + y).mapValues(lambda x: 0.15 + 0.85 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe3fcc-3952-4d61-b33c-c98a920cc006",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksDF = ranksRDD.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a426cf-2ac4-454c-92fc-2b408bc3d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksDF.write.format(\"csv\").save(\"hdfs://172.31.75.157:9000/ranksDF_pt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45f189-9852-48ce-b57e-073cefa6a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 50 URLs by rank\n",
    "top_50 = ranksRDD.takeOrdered(50, key=lambda x: -x[1])\n",
    "# Print the top 50\n",
    "for url, rank in top_50:\n",
    "    print(f\"URL: {url}, Rank: {rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321f8b0-d187-4925-ad51-d94f1d76904b",
   "metadata": {},
   "source": [
    "**Task 3**\n",
    "Kill a Worker process and see the changes. You should trigger the failure to a selected worker VM when the application reaches anywhere between 25% to 75% of its lifetime (hint: use the Spark Jobs web interface to track the detailed job execution progress):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b1e73-14f4-4450-a7e7-30075f04d832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fd75d25-a943-4437-aaa4-2d5ed2b09cf3",
   "metadata": {},
   "source": [
    "From a shell, clear the memory cache using sudo sh -c \"sync; echo 3 > /proc/sys/vm/drop_caches\" on vm2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a7204-deb8-428b-8755-841de74f3167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f71d133d-967d-40e0-953b-135765d85c30",
   "metadata": {},
   "source": [
    "In your shell, kill the Worker process on vm2: To do so, use jps to get the process ID (PID) of the Spark Worker on vm2 and then use the command kill -9 <Worker_PID> to kill the Spark Worker process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330b91d-69d3-4c84-9838-a0df6dd1e386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1296f8d8-aa7f-408c-ae78-9a844bd70df2",
   "metadata": {},
   "source": [
    "For the computed result: your PageRank application should print the first 50 rows with the highest ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae738674-b2d6-491e-b423-a62375e2d75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
