{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972d117d-4640-42cd-86f7-64c99ed9fed9",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "In this part, you will implement a simple Spark application. We have provided some sample data collected at this link (using wget). Download the file to your home directory of vm1.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e1abaa-1816-41e2-ae75-38eeef42f49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/spark-3.3.1-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark as fs\n",
    "fs.init('/home/ubuntu/spark-3.3.1-bin-hadoop3')\n",
    "fs.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4813edd0-f149-4435-806e-e19e43bece31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/27 10:11:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"DS5110\")\n",
    "            .master(\"spark://172.31.75.157:7077\")\n",
    "            .config(\"spark.executor.memory\", \"1024M\")\n",
    "            .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8769f24d-16d1-4efa-9927-de43096eaf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"hdfs://172.31.75.157:9000/export.csv\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507510c2-8baa-43b3-b7ae-f689e664d27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|battery_level|\n",
      "+-------------+\n",
      "|            8|\n",
      "|            7|\n",
      "|            2|\n",
      "|            6|\n",
      "|            4|\n",
      "|            3|\n",
      "|            3|\n",
      "|            0|\n",
      "|            3|\n",
      "|            7|\n",
      "|            3|\n",
      "|            0|\n",
      "|            6|\n",
      "|            1|\n",
      "|            9|\n",
      "|            4|\n",
      "|            0|\n",
      "|            4|\n",
      "|            9|\n",
      "|            7|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"battery_level\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1076d3-0bdb-4452-9860-8de65b2105ef",
   "metadata": {},
   "source": [
    "You then need to sort the data firstly by the country code alphabetically (the third column ccr2) then by the timestamp (the last column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0747a85-6b7f-4edb-bfdc-d4a5100e0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.orderBy(\"cca2\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "565e5fd2-71aa-49f3-a892-3d1a0c4b001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+----+--------------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|                  cn|device_id|         device_name|humidity|             ip|latitude|   lcd|longitude|  scale|temp|    timestamp|\n",
      "+-------------+---------+----+----+--------------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "|            5|     1217|  AE| ARE|United Arab Emirates|      501|device-mac-501e4O...|      48|  213.42.16.154|    24.0|yellow|     54.0|Celsius|  16|1458444054343|\n",
      "|            0|      915|  AR| ARG|           Argentina|      227|meter-gauge-2273p...|      34|  200.71.230.81|   -34.6| green|   -58.38|Celsius|  15|1458444054251|\n",
      "|            1|     1189|  AR| ARG|           Argentina|      319|meter-gauge-319Y3...|      54| 200.71.236.145|   -34.6|yellow|   -58.38|Celsius|  25|1458444054287|\n",
      "|            8|     1386|  AR| ARG|           Argentina|      763|meter-gauge-763JW...|      82|    200.55.0.70|   -34.6|yellow|   -58.38|Celsius|  21|1458444054404|\n",
      "|            0|      861|  AR| ARG|           Argentina|      943|meter-gauge-943BT...|      77|  200.59.128.19|   -34.6| green|   -58.38|Celsius|  33|1458444054435|\n",
      "|            5|      939|  AT| AUT|             Austria|       21|  device-mac-21sjz5h|      44|193.200.142.254|    48.2| green|    16.37|Celsius|  30|1458444054131|\n",
      "|            6|     1328|  AT| AUT|             Austria|       75|device-mac-75OLmC...|      96| 143.161.246.65|    48.2|yellow|    16.37|Celsius|  12|1458444054168|\n",
      "|            8|     1287|  AT| AUT|             Austria|      236|sensor-pad-2369xz...|      47|  217.25.119.17|    48.2|yellow|    16.37|Celsius|  22|1458444054256|\n",
      "|            2|     1522|  AT| AUT|             Austria|      257|meter-gauge-257AT...|      26|   87.243.133.1|    47.2|   red|    14.83|Celsius|  16|1458444054266|\n",
      "|            1|      811|  AT| AUT|             Austria|      271|meter-gauge-271BjIL0|      31|  149.148.140.1|    48.2| green|    16.37|Celsius|  16|1458444054271|\n",
      "|            7|      904|  AT| AUT|             Austria|      294|sensor-pad-294FMZ...|      26|     83.65.45.1|    48.2| green|    16.37|Celsius|  14|1458444054279|\n",
      "|            6|      917|  AT| AUT|             Austria|      369|device-mac-369rYH...|      25|  193.239.188.1|    48.2| green|    16.37|Celsius|  23|1458444054303|\n",
      "|            3|      826|  AT| AUT|             Austria|      483|device-mac-483Tyi...|      90| 84.116.245.201|    48.2| green|    16.37|Celsius|  16|1458444054339|\n",
      "|            2|      816|  AT| AUT|             Austria|      504|sensor-pad-504Kdi...|      78| 87.243.151.193|   47.27| green|     11.4|Celsius|  32|1458444054344|\n",
      "|            1|     1196|  AT| AUT|             Austria|      585|device-mac-5851AntHC|      51|   84.116.252.9|    48.2|yellow|    16.37|Celsius|  27|1458444054364|\n",
      "|            4|     1042|  AT| AUT|             Austria|      758| sensor-pad-7589QBtr|      48|   62.218.4.130|    48.2|yellow|    16.37|Celsius|  30|1458444054403|\n",
      "|            5|     1543|  AT| AUT|             Austria|      767|meter-gauge-767rd...|      65|  195.222.121.1|    48.2|   red|    16.37|Celsius|  19|1458444054405|\n",
      "|            0|      941|  AT| AUT|             Austria|      974|sensor-pad-974x9dkX1|      53| 84.116.216.166|    48.2| green|    16.37|Celsius|  26|1458444054439|\n",
      "|            8|      895|  AT| AUT|             Austria|      977|meter-gauge-977yB...|      52|     83.65.95.1|    48.2| green|    16.37|Celsius|  11|1458444054440|\n",
      "|            0|      899|  AU| AUS|           Australia|      111|device-mac-111WYt...|      32| 203.123.94.193|   -27.0| green|    133.0|Celsius|  16|1458444054189|\n",
      "+-------------+---------+----+----+--------------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aeab1d9-ff8e-4a50-9ef7-4b361385ff96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_df.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.75.157:9000/new_df_hdfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e38c7c-6bd1-4db1-a68c-f28e51606a7d",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "In this part, you will implement the PageRank algorithm (§2.1) (or the Wikipedia version), which is an algorithm used by search engines like Google to evaluate the quality of links to a webpage. The algorithm can be summarized as follows:\n",
    "\n",
    "1. Set initial rank of each page to be 1.\n",
    "2. On each iteration, each page p contributes to its outgoing neighbors a value of rank(p)/(# of outgoing neighbors of p).\n",
    "3. Update each page’s rank to be 0.15 + 0.85 * (sum of contributions).\n",
    "4. Go to next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad4c6cc-0e6a-4032-97de-ac54f56d554c",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "Write a PySpark application that implements the PageRank algorithm. Your PageRank application should output the following two results: 1) print the first 50 rows with the highest ranks; 2) save the computed results as a Spark DF to HDFS as an HDFS csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "722cdc7b-bde8-414f-8c39-995847783705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import Iterable, Tuple\n",
    "from pyspark.resultiterable import ResultIterable\n",
    "import time\n",
    "\n",
    "def calculateRankContrib(nodes: ResultIterable[str], rank: float) -> Iterable[Tuple[str, float]]:\n",
    "    # number of neighboring nodes the specified node has\n",
    "    num_neighbors = len(nodes)\n",
    "    for each_node in nodes:\n",
    "\t# a tuple, element 1==the node\n",
    "\t# element 2==contribution\n",
    "\t# contribution is the node's current rank/number of neighbors the node has\n",
    "        yield (each_node, rank / num_neighbors)\n",
    "\n",
    "# takes a line from RDD (in this case two nodes)\n",
    "# and turns into a tuple: Tuple\n",
    "# using whitespace as the separator: split\n",
    "# ex: 324     43798 --> (324, 43798)\n",
    "def getNeighborNodes(nodes: str) -> Tuple[str, str]:\n",
    "    parts = re.split(r'\\s+', nodes)\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "# get data\n",
    "linesRDD = spark.sparkContext.textFile(\"hdfs://172.31.75.157:9000/web-BerkStan.txt\")\n",
    "# clean it up a little\n",
    "linesRDD = (\n",
    "    linesRDD\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda x: x[1] >= 4)\n",
    "    .map(lambda x: x[0])\n",
    ")\n",
    "\n",
    "# Number of iterations\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75537453-3a30-4fce-8473-654e63395f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 172 ms, sys: 12.8 ms, total: 185 ms\n",
      "Wall time: 535 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# applies the func. to each line of RDD\n",
    "# for each line, get a tuple of (current node, neighbor node): lambda\n",
    "# make that into key, value pair: map\n",
    "# drop dups: distinct\n",
    "# group by key (current node) so that each unique node (key) has group of neighbor nodes (value): groupby\n",
    "nodesRDD = linesRDD.map(lambda nodes: getNeighborNodes(nodes)).distinct().groupByKey()\n",
    "# each line of nodes==(node, iterable(neighbors))\n",
    "\n",
    "# Initialize a ranks RDD\n",
    "# for each tuple of nodes:\n",
    "# take each node and set rank==1\n",
    "ranksRDD = nodesRDD.map(lambda key: (key[0], 1.0))\n",
    "# each line of rank==(node, 1)\n",
    "\n",
    "# Calculates and updates node ranks continuously using PageRank algorithm\n",
    "for iteration in range(N):\n",
    "    # combine nodes with ranks==>(node, (iterable(neighbors), rank)): join\n",
    "    # flatMap allows access to each key-val to apply lambda\n",
    "    # where x=(node, (iterable(neighbors), rank))\n",
    "    contributions = nodesRDD.join(ranksRDD).flatMap(lambda x: calculateRankContrib(x[1][0], x[1][1]))\n",
    "    # each line of contrib==(node, contribution of node)\n",
    "    \n",
    "    # Update ranks based on contributions\n",
    "    # sum(all contribs) for each node: reduce\n",
    "    # mapValues allows output of lambda to be used as input for next lambda\n",
    "    ranksRDD = contributions.reduceByKey(lambda x, y: x + y).mapValues(lambda x: 0.15 + 0.85 * x)\n",
    "    # each line of rank==(node, new rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaff84da-ec99-49e1-8bdd-dd5898477413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:=====================================================>  (21 + 1) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 670 ms, sys: 114 ms, total: 784 ms\n",
      "Wall time: 13min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranksDF = ranksRDD.toDF()\n",
    "ranksDF.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.75.157:9000/ranksDF_pt1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e286243-974f-4d30-a65e-87e5248b2526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 77:==================================================>     (20 + 2) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 272919, Rank: 6531.324623752465\n",
      "Node: 438238, Rank: 4335.32315856444\n",
      "Node: 571448, Rank: 2383.8976074118887\n",
      "Node: 601656, Rank: 2195.3940755967283\n",
      "Node: 316792, Rank: 1855.6908757901508\n",
      "Node: 319209, Rank: 1632.8193684975686\n",
      "Node: 184094, Rank: 1532.28423744834\n",
      "Node: 571447, Rank: 1492.9301630938783\n",
      "Node: 401873, Rank: 1436.1600933469288\n",
      "Node: 66244, Rank: 1261.578395867334\n",
      "Node: 68949, Rank: 1260.791942134913\n",
      "Node: 284306, Rank: 1257.2475650644853\n",
      "Node: 68948, Rank: 1251.172353645922\n",
      "Node: 96070, Rank: 1235.298540597625\n",
      "Node: 77284, Rank: 1235.298540597625\n",
      "Node: 68946, Rank: 1235.298540597625\n",
      "Node: 95551, Rank: 1235.298540597625\n",
      "Node: 66909, Rank: 1235.2985405976249\n",
      "Node: 95552, Rank: 1235.2985405976247\n",
      "Node: 86238, Rank: 1235.2985405976247\n",
      "Node: 86239, Rank: 1235.2985405976247\n",
      "Node: 86237, Rank: 1235.2985405976247\n",
      "Node: 68947, Rank: 1235.2985405976247\n",
      "Node: 768, Rank: 1225.5975665113074\n",
      "Node: 927, Rank: 1117.8383051141836\n",
      "Node: 210376, Rank: 920.6701252803678\n",
      "Node: 95527, Rank: 919.6797146521088\n",
      "Node: 100130, Rank: 916.0190658202686\n",
      "Node: 101163, Rank: 912.5380530105947\n",
      "Node: 95018, Rank: 911.1831080077988\n",
      "Node: 100646, Rank: 909.7095673033012\n",
      "Node: 96045, Rank: 904.3981315809748\n",
      "Node: 66879, Rank: 895.7909746044763\n",
      "Node: 210305, Rank: 893.0386730972408\n",
      "Node: 319412, Rank: 887.9352083382672\n",
      "Node: 571451, Rank: 875.7852546255609\n",
      "Node: 570985, Rank: 871.5825582573226\n",
      "Node: 544858, Rank: 869.6096568148241\n",
      "Node: 184142, Rank: 863.2307781841786\n",
      "Node: 299039, Rank: 832.3149809807293\n",
      "Node: 49176, Rank: 819.8687801616516\n",
      "Node: 299040, Rank: 784.9195782082287\n",
      "Node: 319210, Rank: 764.4429282969843\n",
      "Node: 184332, Rank: 748.1100966771171\n",
      "Node: 184279, Rank: 743.4092370378002\n",
      "Node: 743, Rank: 694.5573570089946\n",
      "Node: 313077, Rank: 681.9298001499118\n",
      "Node: 331840, Rank: 665.4905257656997\n",
      "Node: 33, Rank: 660.9927237591631\n",
      "Node: 184150, Rank: 649.4401909507083\n",
      "CPU times: user 23.3 ms, sys: 3.36 ms, total: 26.7 ms\n",
      "Wall time: 4.33 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# top 50 nodes by rank\n",
    "top_50 = ranksRDD.takeOrdered(50, key=lambda x: -x[1])\n",
    "# Print the top 50\n",
    "for node, rank in top_50:\n",
    "    print(f\"Node: {node}, Rank: {rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c307e58-538e-42a7-baff-55f37d65f3e8",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "In order to achieve high parallelism, Spark will split the data into smaller chunks called partitions, which are distributed across different nodes in the cluster. Partitions can be changed in several ways. For example, any shuffle operation on an RDD (e.g., join()) will result in a change in partitions (customizable via user’s configuration). In addition, one can also decide how to partition data when creating/configuring RDDs (hint: e.g., you can use the function partitionBy()). For this task, add appropriate custom RDD partitioning and see what changes. For the computed result: your PageRank application should print the first 50 rows with the highest ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a211ae44-21b2-4731-b2bc-483727d94bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 155 ms, sys: 8.86 ms, total: 164 ms\n",
      "Wall time: 476 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# same as before but use partitionBy\n",
    "# applies the func. to each line of RDD\n",
    "# for each line, get a tuple of (current node, neighbor node): lambda\n",
    "# make that into key, value pair: map\n",
    "# drop dups: distinct\n",
    "# group by key (current node) so that each unique node (key) has group of neighbor nodes (value): groupby\n",
    "nodesRDD = linesRDD.map(lambda nodes: getNeighborNodes(nodes)).distinct().groupByKey().partitionBy(4)\n",
    "# each line of nodes==(node, iterable(neighbors))\n",
    "\n",
    "# Initialize a ranks RDD\n",
    "# for each tuple of nodes:\n",
    "# take each node and set rank==1\n",
    "ranksRDD = nodesRDD.map(lambda key: (key[0], 1.0))\n",
    "# each line of rank==(node, 1)\n",
    "\n",
    "# Calculates and updates node ranks continuously using PageRank algorithm\n",
    "for iteration in range(N):\n",
    "    # combine nodes with ranks==>(node, (iterable(neighbors), rank)): join\n",
    "    # flatMap allows access to each key-val to apply lambda\n",
    "    # where x=(node, (iterable(neighbors), rank))\n",
    "    contributions = nodesRDD.join(ranksRDD).flatMap(lambda x: calculateRankContrib(x[1][0], x[1][1]))\n",
    "    # each line of contrib==(node, contribution of node)\n",
    "    \n",
    "    # Update ranks based on contributions\n",
    "    # sum(all contribs) for each node: reduce\n",
    "    # mapValues allows output of lambda to be used as input for next lambda\n",
    "    ranksRDD = contributions.reduceByKey(lambda x, y: x + y).mapValues(lambda x: 0.15 + 0.85 * x)\n",
    "    # each line of rank==(node, new rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46fe3fcc-3952-4d61-b33c-c98a920cc006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 519 ms, sys: 134 ms, total: 653 ms\n",
      "Wall time: 8min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranksDF = ranksRDD.toDF()\n",
    "ranksDF.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.75.157:9000/ranksDF_pt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e45f189-9852-48ce-b57e-073cefa6a953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 149:==================================================>    (40 + 2) / 44]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 272919, Rank: 6531.324623752434\n",
      "Node: 438238, Rank: 4335.323158564441\n",
      "Node: 571448, Rank: 2383.8976074118855\n",
      "Node: 601656, Rank: 2195.394075596729\n",
      "Node: 316792, Rank: 1855.6908757901422\n",
      "Node: 319209, Rank: 1632.819368497569\n",
      "Node: 184094, Rank: 1532.284237448333\n",
      "Node: 571447, Rank: 1492.930163093877\n",
      "Node: 401873, Rank: 1436.1600933469256\n",
      "Node: 66244, Rank: 1261.578395867336\n",
      "Node: 68949, Rank: 1260.7919421349152\n",
      "Node: 284306, Rank: 1257.2475650644835\n",
      "Node: 68948, Rank: 1251.1723536459244\n",
      "Node: 95552, Rank: 1235.2985405976272\n",
      "Node: 77284, Rank: 1235.2985405976272\n",
      "Node: 86237, Rank: 1235.2985405976272\n",
      "Node: 95551, Rank: 1235.2985405976272\n",
      "Node: 96070, Rank: 1235.2985405976272\n",
      "Node: 86238, Rank: 1235.2985405976272\n",
      "Node: 68946, Rank: 1235.2985405976272\n",
      "Node: 86239, Rank: 1235.2985405976272\n",
      "Node: 66909, Rank: 1235.2985405976272\n",
      "Node: 68947, Rank: 1235.298540597627\n",
      "Node: 768, Rank: 1225.5975665113017\n",
      "Node: 927, Rank: 1117.8383051141802\n",
      "Node: 210376, Rank: 920.6701252803681\n",
      "Node: 95527, Rank: 919.6797146521111\n",
      "Node: 100130, Rank: 916.0190658202707\n",
      "Node: 101163, Rank: 912.5380530105969\n",
      "Node: 95018, Rank: 911.1831080078009\n",
      "Node: 100646, Rank: 909.7095673033035\n",
      "Node: 96045, Rank: 904.3981315809767\n",
      "Node: 66879, Rank: 895.7909746044784\n",
      "Node: 210305, Rank: 893.0386730972414\n",
      "Node: 319412, Rank: 887.9352083382672\n",
      "Node: 571451, Rank: 875.7852546255604\n",
      "Node: 570985, Rank: 871.5825582573215\n",
      "Node: 544858, Rank: 869.6096568148241\n",
      "Node: 184142, Rank: 863.2307781841763\n",
      "Node: 299039, Rank: 832.3149809807286\n",
      "Node: 49176, Rank: 819.86878016165\n",
      "Node: 299040, Rank: 784.9195782082278\n",
      "Node: 319210, Rank: 764.4429282969841\n",
      "Node: 184332, Rank: 748.1100966771153\n",
      "Node: 184279, Rank: 743.4092370377983\n",
      "Node: 743, Rank: 694.5573570089931\n",
      "Node: 313077, Rank: 681.9298001499116\n",
      "Node: 331840, Rank: 665.4905257656994\n",
      "Node: 33, Rank: 660.992723759162\n",
      "Node: 184150, Rank: 649.4401909507037\n",
      "CPU times: user 32.1 ms, sys: 3.54 ms, total: 35.6 ms\n",
      "Wall time: 5.11 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# top 50 nodes by rank\n",
    "top_50 = ranksRDD.takeOrdered(50, key=lambda x: -x[1])\n",
    "# Print the top 50\n",
    "for node, rank in top_50:\n",
    "    print(f\"Node: {node}, Rank: {rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0cd5ea7-753d-4483-b81c-3630c7dee9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 112 ms, sys: 21.3 ms, total: 134 ms\n",
      "Wall time: 357 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# same as before but use partitionBy\n",
    "# applies the func. to each line of RDD\n",
    "# for each line, get a tuple of (current node, neighbor node): lambda\n",
    "# make that into key, value pair: map\n",
    "# drop dups: distinct\n",
    "# group by key (current node) so that each unique node (key) has group of neighbor nodes (value): groupby\n",
    "nodesRDD = linesRDD.map(lambda nodes: getNeighborNodes(nodes)).distinct().groupByKey().partitionBy(8)\n",
    "# each line of nodes==(node, iterable(neighbors))\n",
    "\n",
    "# Initialize a ranks RDD\n",
    "# for each tuple of nodes:\n",
    "# take each node and set rank==1\n",
    "ranksRDD = nodesRDD.map(lambda key: (key[0], 1.0))\n",
    "# each line of rank==(node, 1)\n",
    "\n",
    "# Calculates and updates node ranks continuously using PageRank algorithm\n",
    "for iteration in range(N):\n",
    "    # combine nodes with ranks==>(node, (iterable(neighbors), rank)): join\n",
    "    # flatMap allows access to each key-val to apply lambda\n",
    "    # where x=(node, (iterable(neighbors), rank))\n",
    "    contributions = nodesRDD.join(ranksRDD).flatMap(lambda x: calculateRankContrib(x[1][0], x[1][1]))\n",
    "    # each line of contrib==(node, contribution of node)\n",
    "    \n",
    "    # Update ranks based on contributions\n",
    "    # sum(all contribs) for each node: reduce\n",
    "    # mapValues allows output of lambda to be used as input for next lambda\n",
    "    ranksRDD = contributions.reduceByKey(lambda x, y: x + y).mapValues(lambda x: 0.15 + 0.85 * x)\n",
    "    # each line of rank==(node, new rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db72f9aa-3938-4117-9f9e-c1fc4bb2a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 582 ms, sys: 142 ms, total: 723 ms\n",
      "Wall time: 6min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranksDF = ranksRDD.toDF()\n",
    "ranksDF.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.75.157:9000/ranksDF_pt2.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68c4f238-06ba-4737-9e56-2f034bfbb37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 221:=====================================================> (86 + 2) / 88]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 272919, Rank: 6531.324623752432\n",
      "Node: 438238, Rank: 4335.3231585644335\n",
      "Node: 571448, Rank: 2383.8976074118877\n",
      "Node: 601656, Rank: 2195.394075596731\n",
      "Node: 316792, Rank: 1855.6908757901422\n",
      "Node: 319209, Rank: 1632.8193684975693\n",
      "Node: 184094, Rank: 1532.284237448322\n",
      "Node: 571447, Rank: 1492.9301630938774\n",
      "Node: 401873, Rank: 1436.1600933469194\n",
      "Node: 66244, Rank: 1261.5783958673335\n",
      "Node: 68949, Rank: 1260.791942134913\n",
      "Node: 284306, Rank: 1257.2475650644835\n",
      "Node: 68948, Rank: 1251.172353645922\n",
      "Node: 86239, Rank: 1235.298540597625\n",
      "Node: 95551, Rank: 1235.298540597625\n",
      "Node: 68946, Rank: 1235.298540597625\n",
      "Node: 66909, Rank: 1235.2985405976249\n",
      "Node: 95552, Rank: 1235.2985405976247\n",
      "Node: 77284, Rank: 1235.2985405976247\n",
      "Node: 86237, Rank: 1235.2985405976247\n",
      "Node: 68947, Rank: 1235.2985405976247\n",
      "Node: 96070, Rank: 1235.2985405976247\n",
      "Node: 86238, Rank: 1235.2985405976247\n",
      "Node: 768, Rank: 1225.5975665112949\n",
      "Node: 927, Rank: 1117.8383051141748\n",
      "Node: 210376, Rank: 920.6701252803691\n",
      "Node: 95527, Rank: 919.6797146521088\n",
      "Node: 100130, Rank: 916.0190658202682\n",
      "Node: 101163, Rank: 912.5380530105945\n",
      "Node: 95018, Rank: 911.1831080077985\n",
      "Node: 100646, Rank: 909.7095673033012\n",
      "Node: 96045, Rank: 904.3981315809746\n",
      "Node: 66879, Rank: 895.7909746044763\n",
      "Node: 210305, Rank: 893.0386730972417\n",
      "Node: 319412, Rank: 887.9352083382674\n",
      "Node: 571451, Rank: 875.7852546255604\n",
      "Node: 570985, Rank: 871.5825582573215\n",
      "Node: 544858, Rank: 869.609656814824\n",
      "Node: 184142, Rank: 863.2307781841728\n",
      "Node: 299039, Rank: 832.3149809807288\n",
      "Node: 49176, Rank: 819.8687801616508\n",
      "Node: 299040, Rank: 784.9195782082274\n",
      "Node: 319210, Rank: 764.4429282969843\n",
      "Node: 184332, Rank: 748.1100966771137\n",
      "Node: 184279, Rank: 743.409237037797\n",
      "Node: 743, Rank: 694.5573570089921\n",
      "Node: 313077, Rank: 681.9298001499111\n",
      "Node: 331840, Rank: 665.4905257656992\n",
      "Node: 33, Rank: 660.992723759161\n",
      "Node: 184150, Rank: 649.4401909506953\n",
      "CPU times: user 34.4 ms, sys: 7.97 ms, total: 42.4 ms\n",
      "Wall time: 4.85 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# top 50 nodes by rank\n",
    "top_50 = ranksRDD.takeOrdered(50, key=lambda x: -x[1])\n",
    "# Print the top 50\n",
    "for node, rank in top_50:\n",
    "    print(f\"Node: {node}, Rank: {rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78efddb4-3c1e-49d5-b306-808309d20640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95.1 ms, sys: 16.4 ms, total: 112 ms\n",
      "Wall time: 322 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# same as before but use partitionBy\n",
    "# applies the func. to each line of RDD\n",
    "# for each line, get a tuple of (current node, neighbor node): lambda\n",
    "# make that into key, value pair: map\n",
    "# drop dups: distinct\n",
    "# group by key (current node) so that each unique node (key) has group of neighbor nodes (value): groupby\n",
    "nodesRDD = linesRDD.map(lambda nodes: getNeighborNodes(nodes)).distinct().groupByKey().partitionBy(4)\n",
    "# each line of nodes==(node, iterable(neighbors))\n",
    "\n",
    "# Initialize a ranks RDD\n",
    "# for each tuple of nodes:\n",
    "# take each node and set rank==1\n",
    "ranksRDD = nodesRDD.map(lambda key: (key[0], 1.0)).partitionBy(4)\n",
    "# each line of rank==(node, 1)\n",
    "\n",
    "# Calculates and updates node ranks continuously using PageRank algorithm\n",
    "for iteration in range(N):\n",
    "    # combine nodes with ranks==>(node, (iterable(neighbors), rank)): join\n",
    "    # flatMap allows access to each key-val to apply lambda\n",
    "    # where x=(node, (iterable(neighbors), rank))\n",
    "    contributions = nodesRDD.join(ranksRDD).flatMap(lambda x: calculateRankContrib(x[1][0], x[1][1]))\n",
    "    # each line of contrib==(node, contribution of node)\n",
    "    \n",
    "    # Update ranks based on contributions\n",
    "    # sum(all contribs) for each node: reduce\n",
    "    # mapValues allows output of lambda to be used as input for next lambda\n",
    "    ranksRDD = contributions.reduceByKey(lambda x, y: x + y).mapValues(lambda x: 0.15 + 0.85 * x).partitionBy(4)\n",
    "    # each line of rank==(node, new rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f94b97ba-aeb8-49ac-ba8e-1d943db08cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 251:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 101 ms, sys: 32.3 ms, total: 133 ms\n",
      "Wall time: 4min 38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranksDF = ranksRDD.toDF()\n",
    "ranksDF.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.75.157:9000/ranksDF_pt2.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71974d5a-7fa4-4835-85b9-45fb8eb9b863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 266:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 272919, Rank: 6531.324623752454\n",
      "Node: 438238, Rank: 4335.32315856443\n",
      "Node: 571448, Rank: 2383.897607411891\n",
      "Node: 601656, Rank: 2195.3940755967305\n",
      "Node: 316792, Rank: 1855.6908757901538\n",
      "Node: 319209, Rank: 1632.8193684975708\n",
      "Node: 184094, Rank: 1532.2842374483453\n",
      "Node: 571447, Rank: 1492.930163093879\n",
      "Node: 401873, Rank: 1436.1600933469308\n",
      "Node: 66244, Rank: 1261.5783958673362\n",
      "Node: 68949, Rank: 1260.7919421349154\n",
      "Node: 284306, Rank: 1257.2475650644856\n",
      "Node: 68948, Rank: 1251.1723536459224\n",
      "Node: 86237, Rank: 1235.2985405976276\n",
      "Node: 95552, Rank: 1235.2985405976276\n",
      "Node: 86239, Rank: 1235.2985405976276\n",
      "Node: 95551, Rank: 1235.2985405976274\n",
      "Node: 68946, Rank: 1235.2985405976272\n",
      "Node: 68947, Rank: 1235.2985405976272\n",
      "Node: 77284, Rank: 1235.2985405976272\n",
      "Node: 86238, Rank: 1235.298540597627\n",
      "Node: 66909, Rank: 1235.298540597627\n",
      "Node: 96070, Rank: 1235.2985405976256\n",
      "Node: 768, Rank: 1225.5975665113103\n",
      "Node: 927, Rank: 1117.8383051141866\n",
      "Node: 210376, Rank: 920.670125280368\n",
      "Node: 95527, Rank: 919.6797146521084\n",
      "Node: 100130, Rank: 916.0190658202682\n",
      "Node: 101163, Rank: 912.5380530105945\n",
      "Node: 95018, Rank: 911.1831080077983\n",
      "Node: 100646, Rank: 909.7095673033009\n",
      "Node: 96045, Rank: 904.3981315809743\n",
      "Node: 66879, Rank: 895.7909746044759\n",
      "Node: 210305, Rank: 893.038673097241\n",
      "Node: 319412, Rank: 887.935208338267\n",
      "Node: 571451, Rank: 875.7852546255601\n",
      "Node: 570985, Rank: 871.5825582573244\n",
      "Node: 544858, Rank: 869.6096568148223\n",
      "Node: 184142, Rank: 863.2307781841823\n",
      "Node: 299039, Rank: 832.3149809807301\n",
      "Node: 49176, Rank: 819.8687801616533\n",
      "Node: 299040, Rank: 784.9195782082277\n",
      "Node: 319210, Rank: 764.4429282969842\n",
      "Node: 184332, Rank: 748.1100966771198\n",
      "Node: 184279, Rank: 743.4092370378029\n",
      "Node: 743, Rank: 694.5573570089971\n",
      "Node: 313077, Rank: 681.9298001499111\n",
      "Node: 331840, Rank: 665.4905257656994\n",
      "Node: 33, Rank: 660.9927237591646\n",
      "Node: 184150, Rank: 649.4401909507088\n",
      "CPU times: user 9.71 ms, sys: 0 ns, total: 9.71 ms\n",
      "Wall time: 933 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# top 50 nodes by rank\n",
    "top_50 = ranksRDD.takeOrdered(50, key=lambda x: -x[1])\n",
    "# Print the top 50\n",
    "for node, rank in top_50:\n",
    "    print(f\"Node: {node}, Rank: {rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321f8b0-d187-4925-ad51-d94f1d76904b",
   "metadata": {},
   "source": [
    "**Task 3**\n",
    "Kill a Worker process and see the changes. You should trigger the failure to a selected worker VM when the application reaches anywhere between 25% to 75% of its lifetime (hint: use the Spark Jobs web interface to track the detailed job execution progress):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd75d25-a943-4437-aaa4-2d5ed2b09cf3",
   "metadata": {},
   "source": [
    "From a shell, clear the memory cache using sudo sh -c \"sync; echo 3 > /proc/sys/vm/drop_caches\" on vm2;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71d133d-967d-40e0-953b-135765d85c30",
   "metadata": {},
   "source": [
    "In your shell, kill the Worker process on vm2: To do so, use jps to get the process ID (PID) of the Spark Worker on vm2 and then use the command kill -9 <Worker_PID> to kill the Spark Worker process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296f8d8-aa7f-408c-ae78-9a844bd70df2",
   "metadata": {},
   "source": [
    "For the computed result: your PageRank application should print the first 50 rows with the highest ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7cedf0e-571d-4939-ab78-25b35009e83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 166 ms, sys: 14 ms, total: 180 ms\n",
      "Wall time: 529 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# same as before but use partitionBy\n",
    "# applies the func. to each line of RDD\n",
    "# for each line, get a tuple of (current node, neighbor node): lambda\n",
    "# make that into key, value pair: map\n",
    "# drop dups: distinct\n",
    "# group by key (current node) so that each unique node (key) has group of neighbor nodes (value): groupby\n",
    "nodesRDD = linesRDD.map(lambda nodes: getNeighborNodes(nodes)).distinct().groupByKey()\n",
    "# each line of nodes==(node, iterable(neighbors))\n",
    "\n",
    "# Initialize a ranks RDD\n",
    "# for each tuple of nodes:\n",
    "# take each node and set rank==1\n",
    "ranksRDD = nodesRDD.map(lambda key: (key[0], 1.0))\n",
    "# each line of rank==(node, 1)\n",
    "\n",
    "# Calculates and updates node ranks continuously using PageRank algorithm\n",
    "for iteration in range(N):\n",
    "    # combine nodes with ranks==>(node, (iterable(neighbors), rank)): join\n",
    "    # flatMap allows access to each key-val to apply lambda\n",
    "    # where x=(node, (iterable(neighbors), rank))\n",
    "    contributions = nodesRDD.join(ranksRDD).flatMap(lambda x: calculateRankContrib(x[1][0], x[1][1]))\n",
    "    # each line of contrib==(node, contribution of node)\n",
    "    \n",
    "    # Update ranks based on contributions\n",
    "    # sum(all contribs) for each node: reduce\n",
    "    # mapValues allows output of lambda to be used as input for next lambda\n",
    "    ranksRDD = contributions.reduceByKey(lambda x, y: x + y).mapValues(lambda x: 0.15 + 0.85 * x)\n",
    "    # each line of rank==(node, new rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73055c8b-925e-4e1c-b934-ae1116698aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:================================================>      (16 + 2) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/27 10:58:39 ERROR TaskSchedulerImpl: Lost executor 0 on 172.31.72.60: worker lost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:==================================================>  (17 + -15) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/27 10:59:04 WARN TaskSetManager: Lost task 10.1 in stage 283.0 (TID 2377) (172.31.75.157 executor 1): FetchFailed(null, shuffleId=89, mapIndex=-1, mapId=-1, reduceId=8, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 89 partition 8\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1705)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1652)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1651)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1651)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1294)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1256)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      ")\n",
      "24/02/27 10:59:04 WARN TaskSetManager: Lost task 2.1 in stage 283.0 (TID 2378) (172.31.75.157 executor 1): FetchFailed(null, shuffleId=89, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 89 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1705)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1652)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1651)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1651)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1294)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1256)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:=============(18 + -16) / 18][Stage 312:=============> (20 + 2) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 484 ms, sys: 85.4 ms, total: 570 ms\n",
      "Wall time: 15min 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranksDF = ranksRDD.toDF()\n",
    "ranksDF.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.75.157:9000/ranksDF_pt3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e21ee23-a892-44dc-877b-05c22c908b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:=============(18 + -16) / 18][Stage 335:============>  (19 + 2) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 272919, Rank: 6531.324623752465\n",
      "Node: 438238, Rank: 4335.323158564438\n",
      "Node: 571448, Rank: 2383.8976074118887\n",
      "Node: 601656, Rank: 2195.394075596728\n",
      "Node: 316792, Rank: 1855.690875790152\n",
      "Node: 319209, Rank: 1632.8193684975686\n",
      "Node: 184094, Rank: 1532.28423744834\n",
      "Node: 571447, Rank: 1492.9301630938787\n",
      "Node: 401873, Rank: 1436.1600933469285\n",
      "Node: 66244, Rank: 1261.5783958673337\n",
      "Node: 68949, Rank: 1260.791942134913\n",
      "Node: 284306, Rank: 1257.2475650644851\n",
      "Node: 68948, Rank: 1251.1723536459222\n",
      "Node: 96070, Rank: 1235.298540597625\n",
      "Node: 86239, Rank: 1235.298540597625\n",
      "Node: 86237, Rank: 1235.298540597625\n",
      "Node: 95551, Rank: 1235.298540597625\n",
      "Node: 95552, Rank: 1235.2985405976249\n",
      "Node: 86238, Rank: 1235.2985405976249\n",
      "Node: 77284, Rank: 1235.2985405976249\n",
      "Node: 68946, Rank: 1235.2985405976249\n",
      "Node: 66909, Rank: 1235.2985405976249\n",
      "Node: 68947, Rank: 1235.2985405976249\n",
      "Node: 768, Rank: 1225.5975665113076\n",
      "Node: 927, Rank: 1117.8383051141843\n",
      "Node: 210376, Rank: 920.6701252803678\n",
      "Node: 95527, Rank: 919.6797146521088\n",
      "Node: 100130, Rank: 916.0190658202683\n",
      "Node: 101163, Rank: 912.5380530105945\n",
      "Node: 95018, Rank: 911.1831080077985\n",
      "Node: 100646, Rank: 909.7095673033012\n",
      "Node: 96045, Rank: 904.3981315809746\n",
      "Node: 66879, Rank: 895.7909746044761\n",
      "Node: 210305, Rank: 893.0386730972408\n",
      "Node: 319412, Rank: 887.9352083382672\n",
      "Node: 571451, Rank: 875.7852546255615\n",
      "Node: 570985, Rank: 871.5825582573231\n",
      "Node: 544858, Rank: 869.609656814824\n",
      "Node: 184142, Rank: 863.2307781841786\n",
      "Node: 299039, Rank: 832.3149809807294\n",
      "Node: 49176, Rank: 819.8687801616512\n",
      "Node: 299040, Rank: 784.9195782082287\n",
      "Node: 319210, Rank: 764.4429282969843\n",
      "Node: 184332, Rank: 748.1100966771173\n",
      "Node: 184279, Rank: 743.4092370378003\n",
      "Node: 743, Rank: 694.5573570089949\n",
      "Node: 313077, Rank: 681.9298001499117\n",
      "Node: 331840, Rank: 665.4905257656995\n",
      "Node: 33, Rank: 660.9927237591636\n",
      "Node: 184150, Rank: 649.4401909507088\n",
      "CPU times: user 15.4 ms, sys: 10.3 ms, total: 25.7 ms\n",
      "Wall time: 3.75 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:=====================================================(18 + -16) / 18]\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# top 50 nodes by rank\n",
    "top_50 = ranksRDD.takeOrdered(50, key=lambda x: -x[1])\n",
    "# Print the top 50\n",
    "for node, rank in top_50:\n",
    "    print(f\"Node: {node}, Rank: {rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f4cff3d-6d7f-44c1-bfba-935a365c8f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 110 ms, sys: 32.3 ms, total: 143 ms\n",
      "Wall time: 325 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# same as before but use partitionBy\n",
    "# applies the func. to each line of RDD\n",
    "# for each line, get a tuple of (current node, neighbor node): lambda\n",
    "# make that into key, value pair: map\n",
    "# drop dups: distinct\n",
    "# group by key (current node) so that each unique node (key) has group of neighbor nodes (value): groupby\n",
    "nodesRDD = linesRDD.map(lambda nodes: getNeighborNodes(nodes)).distinct().groupByKey().partitionBy(4)\n",
    "# each line of nodes==(node, iterable(neighbors))\n",
    "\n",
    "# Initialize a ranks RDD\n",
    "# for each tuple of nodes:\n",
    "# take each node and set rank==1\n",
    "ranksRDD = nodesRDD.map(lambda key: (key[0], 1.0)).partitionBy(4)\n",
    "# each line of rank==(node, 1)\n",
    "\n",
    "# Calculates and updates node ranks continuously using PageRank algorithm\n",
    "for iteration in range(N):\n",
    "    # combine nodes with ranks==>(node, (iterable(neighbors), rank)): join\n",
    "    # flatMap allows access to each key-val to apply lambda\n",
    "    # where x=(node, (iterable(neighbors), rank))\n",
    "    contributions = nodesRDD.join(ranksRDD).flatMap(lambda x: calculateRankContrib(x[1][0], x[1][1]))\n",
    "    # each line of contrib==(node, contribution of node)\n",
    "    \n",
    "    # Update ranks based on contributions\n",
    "    # sum(all contribs) for each node: reduce\n",
    "    # mapValues allows output of lambda to be used as input for next lambda\n",
    "    ranksRDD = contributions.reduceByKey(lambda x, y: x + y).mapValues(lambda x: 0.15 + 0.85 * x).partitionBy(4)\n",
    "    # each line of rank==(node, new rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36964acd-7ea1-4e43-acbd-80404f1b6e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:=============(18 + -16) / 18][Stage 346:>                (0 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/27 11:19:37 ERROR TaskSchedulerImpl: Lost executor 2 on 172.31.72.60: worker lost\n",
      "24/02/27 11:19:37 WARN TaskSetManager: Lost task 1.0 in stage 346.0 (TID 2657) (172.31.72.60 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: worker lost\n",
      "24/02/27 11:19:37 WARN TaskSetManager: Lost task 0.0 in stage 346.0 (TID 2656) (172.31.72.60 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: worker lost\n",
      "24/02/27 11:19:37 WARN TaskSetManager: Lost task 0.1 in stage 346.0 (TID 2658) (172.31.75.157 executor 1): FetchFailed(null, shuffleId=116, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 116 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1705)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1652)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1651)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1651)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1294)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1256)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "\n",
      ")\n",
      "24/02/27 11:19:37 WARN TaskSetManager: Lost task 1.1 in stage 346.0 (TID 2659) (172.31.75.157 executor 1): FetchFailed(null, shuffleId=116, mapIndex=-1, mapId=-1, reduceId=1, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 116 partition 1\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1705)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1652)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1651)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1651)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1294)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1256)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "\n",
      ")\n",
      "24/02/27 11:19:37 WARN TaskSetManager: Lost task 2.0 in stage 346.0 (TID 2660) (172.31.75.157 executor 1): FetchFailed(null, shuffleId=116, mapIndex=-1, mapId=-1, reduceId=2, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 116 partition 2\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1705)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1652)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1651)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1651)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1294)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1256)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:=============(18 + -16) / 18][Stage 365:========>        (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 222 ms, sys: 53.1 ms, total: 275 ms\n",
      "Wall time: 11min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranksDF = ranksRDD.toDF()\n",
    "ranksDF.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.75.157:9000/ranksDF_pt3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f9d7e9a-d02d-465c-8e4c-6c662a2d6509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:=============(18 + -16) / 18][Stage 380:========>        (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 272919, Rank: 6531.324623752456\n",
      "Node: 438238, Rank: 4335.323158564431\n",
      "Node: 571448, Rank: 2383.8976074118887\n",
      "Node: 601656, Rank: 2195.3940755967324\n",
      "Node: 316792, Rank: 1855.6908757901538\n",
      "Node: 319209, Rank: 1632.8193684975706\n",
      "Node: 184094, Rank: 1532.284237448342\n",
      "Node: 571447, Rank: 1492.9301630938805\n",
      "Node: 401873, Rank: 1436.1600933469283\n",
      "Node: 66244, Rank: 1261.5783958673321\n",
      "Node: 68949, Rank: 1260.7919421349113\n",
      "Node: 284306, Rank: 1257.2475650644844\n",
      "Node: 68948, Rank: 1251.17235364592\n",
      "Node: 86239, Rank: 1235.298540597624\n",
      "Node: 86238, Rank: 1235.2985405976237\n",
      "Node: 95551, Rank: 1235.2985405976233\n",
      "Node: 86237, Rank: 1235.298540597623\n",
      "Node: 68946, Rank: 1235.298540597623\n",
      "Node: 66909, Rank: 1235.298540597623\n",
      "Node: 77284, Rank: 1235.298540597623\n",
      "Node: 96070, Rank: 1235.298540597623\n",
      "Node: 68947, Rank: 1235.2985405976228\n",
      "Node: 95552, Rank: 1235.2985405976228\n",
      "Node: 768, Rank: 1225.5975665113071\n",
      "Node: 927, Rank: 1117.8383051141832\n",
      "Node: 210376, Rank: 920.670125280368\n",
      "Node: 95527, Rank: 919.6797146521076\n",
      "Node: 100130, Rank: 916.0190658202672\n",
      "Node: 101163, Rank: 912.5380530105933\n",
      "Node: 95018, Rank: 911.1831080077974\n",
      "Node: 100646, Rank: 909.7095673033001\n",
      "Node: 96045, Rank: 904.3981315809734\n",
      "Node: 66879, Rank: 895.7909746044751\n",
      "Node: 210305, Rank: 893.0386730972411\n",
      "Node: 319412, Rank: 887.935208338267\n",
      "Node: 571451, Rank: 875.78525462556\n",
      "Node: 570985, Rank: 871.5825582573207\n",
      "Node: 544858, Rank: 869.6096568148225\n",
      "Node: 184142, Rank: 863.2307781841798\n",
      "Node: 299039, Rank: 832.3149809807301\n",
      "Node: 49176, Rank: 819.8687801616522\n",
      "Node: 299040, Rank: 784.9195782082284\n",
      "Node: 319210, Rank: 764.4429282969842\n",
      "Node: 184332, Rank: 748.1100966771188\n",
      "Node: 184279, Rank: 743.4092370378016\n",
      "Node: 743, Rank: 694.5573570089952\n",
      "Node: 313077, Rank: 681.9298001499114\n",
      "Node: 331840, Rank: 665.4905257656999\n",
      "Node: 33, Rank: 660.9927237591622\n",
      "Node: 184150, Rank: 649.4401909507083\n",
      "CPU times: user 10.1 ms, sys: 2.04 ms, total: 12.1 ms\n",
      "Wall time: 1.66 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:=====================================================(18 + -16) / 18]\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# top 50 nodes by rank\n",
    "top_50 = ranksRDD.takeOrdered(50, key=lambda x: -x[1])\n",
    "# Print the top 50\n",
    "for node, rank in top_50:\n",
    "    print(f\"Node: {node}, Rank: {rank}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
