{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available. GPU can be used.\")\n",
        "    print(\"Version: \", torch.version.cuda)\n",
        "    print(\"Number of GPUs available: \", torch.cuda.device_count())\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU instead.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHWf0o6eSBEP",
        "outputId": "433af361-3c93-4095-add6-7bc396aa7aef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. GPU can be used.\n",
            "Version:  12.1\n",
            "Number of GPUs available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VlcDIgUUnfT",
        "outputId": "2b5a1c53-0ce8-4af7-fc9b-ba2ce7a73bd8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is only to setup PySpark and Spark NLP on Colab\n",
        "# !wget https://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
        "!wget https://setup.johnsnowlabs.com/colab.sh -O - | bash -s -- -g"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56XUCGYhlhvj",
        "outputId": "0ab39f05-82ed-4640-d0a2-dbc6993703c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-15 09:46:02--  https://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2024-04-15 09:46:02--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1191 (1.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-15 09:46:03 (120 MB/s) - written to stdout [1191/1191]\n",
            "\n",
            "Installing PySpark 3.2.3 and Spark NLP 5.3.3\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 5.3.3\n",
            "Upgrading libcudnn8 to 8.1.0 for GPU\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.4/568.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "import pyspark.sql.functions as F\n",
        "spark = sparknlp.start(gpu=True)\n",
        "\n",
        "print(\"\\nSpark NLP version: {}\".format(sparknlp.version()))\n",
        "print(\"\\nApache Spark version: {}\".format(spark.version))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPcevT0DB7o5",
        "outputId": "9ce0d4fd-d826-43a9-8648-ad13c625ff5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Spark NLP version: 5.3.3\n",
            "\n",
            "Apache Spark version: 3.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.pretrained import ResourceDownloader\n",
        "\n",
        "ResourceDownloader.showPublicModels(annotator = \"SentimentDLModel\", lang = \"en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QMF7ZeRBlZI",
        "outputId": "b537cab1-cb69-43f3-913c-3f5a7718be8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+------+---------+\n",
            "| Model                   | lang | version |\n",
            "+-------------------------+------+---------+\n",
            "| sentimentdl_glove_imdb  |  en  | 2.5.0   |\n",
            "| sentimentdl_use_twitter |  en  | 2.5.0   |\n",
            "| sentimentdl_use_imdb    |  en  | 2.5.0   |\n",
            "| sentimentdl_glove_imdb  |  en  | 2.7.1   |\n",
            "| sentimentdl_use_imdb    |  en  | 2.7.0   |\n",
            "| sentimentdl_use_twitter |  en  | 2.7.1   |\n",
            "+-------------------------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testDataset = spark.read \\\n",
        "    .json(\"/content/drive/My Drive/Colab Notebooks/All_Beauty_5.json\")"
      ],
      "metadata": {
        "id": "lXcZqlRPIkrU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testDataset = testDataset.withColumnRenamed(\"reviewText\", \"text\")\n",
        "testDataset = testDataset.withColumnRenamed(\"overall\", \"label\")\n",
        "testDataset = testDataset.withColumn(\"label\", F.when(testDataset[\"label\"] >=3, 1.0).otherwise(0.0))\n",
        "testDataset = testDataset.select(\"label\", \"text\", \"reviewerID\")"
      ],
      "metadata": {
        "id": "nNkktCsmJ3gC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testDataset.show(5)"
      ],
      "metadata": {
        "id": "2GImgmA3JxnS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d68223a3-4107-4d65-9691-a1993983d647"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------+\n",
            "|label|                text|    reviewerID|\n",
            "+-----+--------------------+--------------+\n",
            "|  1.0|As advertised. Re...|A3CIUOJXQ5VDQ2|\n",
            "|  1.0|Like the oder and...|A3H7T87S984REU|\n",
            "|  0.0|I bought this to ...|A3J034YH7UG4KT|\n",
            "|  1.0|HEY!! I am an Aqu...|A2UEO5XR3598GI|\n",
            "|  1.0|If you ever want ...|A3SFRT223XXWF7|\n",
            "+-----+--------------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required modules and classes\n",
        "from sparknlp.base import DocumentAssembler, Pipeline\n",
        "from sparknlp.annotator import (\n",
        "    UniversalSentenceEncoder,\n",
        "    SentimentDLModel\n",
        ")\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "print(\"\\ndocumentAssembler finished!\")\n",
        "\n",
        "use = UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\")\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        "print(\"\\nuseEmbeddings finished!\")\n",
        "\n",
        "sentimentdl = SentimentDLModel.pretrained(\"sentimentdl_use_twitter\", \"en\")\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"prediction\")\n",
        "print(\"\\nsentimentdl finished!\")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "      stages = [\n",
        "          documentAssembler,\n",
        "          use,\n",
        "          sentimentdl\n",
        "      ])"
      ],
      "metadata": {
        "id": "P8yVeWWHXbHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58d92f9-c0e1-489d-bf76-200b98196479"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "documentAssembler finished!\n",
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n",
            "\n",
            "useEmbeddings finished!\n",
            "sentimentdl_use_twitter download started this may take some time.\n",
            "Approximate size to download 11.4 MB\n",
            "[OK!]\n",
            "\n",
            "sentimentdl finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "GEEZNSpbrgdR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the timer\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "print(\"\\nFitting model...\")\n",
        "\n",
        "start_time = time.time()\n",
        "pipelineModel = pipeline.fit(empty_df)\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"\\nModel fitted.\")\n",
        "\n",
        "# Calculate the total time taken\n",
        "total_time = end_time - start_time\n",
        "print(f\"\\nTotal execution time: {total_time} seconds\")\n",
        "\n",
        "# Start the timer\n",
        "print(\"\\nTesting model...\")\n",
        "start_time = time.time()\n",
        "\n",
        "preds = pipelineModel.transform(testDataset)\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"\\nTesting finished.\")\n",
        "\n",
        "# Calculate the total time taken\n",
        "total_time = end_time - start_time\n",
        "print(f\"\\nTotal execution time: {total_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASiPUhhlbk7t",
        "outputId": "ce8f58a6-436e-4c53-c225-b02b35b3f085"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fitting model...\n",
            "\n",
            "Model fitted.\n",
            "\n",
            "Total execution time: 0.0015628337860107422 seconds\n",
            "\n",
            "Testing model...\n",
            "\n",
            "Testing finished.\n",
            "\n",
            "Total execution time: 0.37982821464538574 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = preds.select(\"label\", \"text\")\n",
        "df2 = preds.select(\n",
        "    F.explode(\n",
        "      F.arrays_zip(\n",
        "        preds.document.result,\n",
        "        preds.prediction.result)).alias(\"cols\")\n",
        ").select(\n",
        "    F.expr(\"cols['0']\").alias(\"text\"),\n",
        "    F.expr(\"cols['1']\").alias(\"prediction\")\n",
        ")\n",
        "df2 = df2.withColumn(\n",
        "    \"prediction\",\n",
        "    F.when((df2[\"prediction\"] == \"positive\") | (df2[\"prediction\"] == \"neutral\"), 1.0).otherwise(0.0)\n",
        ")\n",
        "merge_df = df1.join(df2, (df1.text == df2.text), \"inner\").drop(df2[\"text\"])"
      ],
      "metadata": {
        "id": "5EBUw6Q511a3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(merge_df)\n",
        "\n",
        "# Precision, Recall, and F1-Score\n",
        "precision = evaluator.evaluate(merge_df, {evaluator.metricName: \"precisionByLabel\"})\n",
        "recall = evaluator.evaluate(merge_df, {evaluator.metricName: \"recallByLabel\"})\n",
        "f1 = evaluator.evaluate(merge_df, {evaluator.metricName: \"f1\"})\n",
        "\n",
        "# Create a DataFrame with the metrics\n",
        "metrics_df = spark.createDataFrame([\n",
        "    Row(metric=\"\\nAccuracy\", score=accuracy),\n",
        "    Row(metric=\"\\nPrecision\", score=precision),\n",
        "    Row(metric=\"\\nRecall\", score=recall),\n",
        "    Row(metric=\"\\nF1 Score\", score=f1)\n",
        "])"
      ],
      "metadata": {
        "id": "VOHhCqgKtQ-O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Metrics for Pretrained Pipline\\nTwitter Tweets\")\n",
        "metrics_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X72d-L8Yy8PA",
        "outputId": "6ce1a8d3-1c29-4106-ac5c-ba4473e37093"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics for Pretrained Pipline\n",
            "Twitter Tweets\n",
            "+-----------+-------------------+\n",
            "|     metric|              score|\n",
            "+-----------+-------------------+\n",
            "| \\nAccuracy| 0.7938681814644775|\n",
            "|\\nPrecision|0.11605481306039588|\n",
            "|   \\nRecall| 0.9038208168642952|\n",
            "| \\nF1 Score| 0.8616076446639912|\n",
            "+-----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentimentdl = SentimentDLModel.pretrained(\"sentimentdl_use_imdb\", \"en\")\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"prediction\")\n",
        "print(\"\\nsentimentdl finished!\")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "      stages = [\n",
        "          documentAssembler,\n",
        "          use,\n",
        "          sentimentdl\n",
        "      ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyy-8D1J9IRO",
        "outputId": "b385ce28-c5ff-484d-8e6d-cc108e5f2378"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentimentdl_use_imdb download started this may take some time.\n",
            "Approximate size to download 12 MB\n",
            "[OK!]\n",
            "\n",
            "sentimentdl finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the timer\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "print(\"\\nFitting model...\")\n",
        "\n",
        "start_time = time.time()\n",
        "pipelineModel = pipeline.fit(empty_df)\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"\\nModel fitted.\")\n",
        "\n",
        "# Calculate the total time taken\n",
        "total_time = end_time - start_time\n",
        "print(f\"\\nTotal execution time: {total_time} seconds\")\n",
        "\n",
        "# Start the timer\n",
        "print(\"\\nTesting model...\")\n",
        "start_time = time.time()\n",
        "\n",
        "preds = pipelineModel.transform(testDataset)\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"\\nTesting finished.\")\n",
        "\n",
        "# Calculate the total time taken\n",
        "total_time = end_time - start_time\n",
        "print(f\"\\nTotal execution time: {total_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgEMZb2k9PUz",
        "outputId": "e7a94f4a-fa8b-4c1e-8fca-156b3f6e605f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fitting model...\n",
            "\n",
            "Model fitted.\n",
            "\n",
            "Total execution time: 0.0003399848937988281 seconds\n",
            "\n",
            "Testing model...\n",
            "\n",
            "Testing finished.\n",
            "\n",
            "Total execution time: 0.10997390747070312 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = preds.select(\"label\", \"text\")\n",
        "df2 = preds.select(\n",
        "    F.explode(\n",
        "      F.arrays_zip(\n",
        "        preds.document.result,\n",
        "        preds.prediction.result)).alias(\"cols\")\n",
        ").select(\n",
        "    F.expr(\"cols['0']\").alias(\"text\"),\n",
        "    F.expr(\"cols['1']\").alias(\"prediction\")\n",
        ")\n",
        "df2 = df2.withColumn(\n",
        "    \"prediction\",\n",
        "    F.when((df2[\"prediction\"] == \"positive\") | (df2[\"prediction\"] == \"neutral\"), 1.0).otherwise(0.0)\n",
        ")\n",
        "merge_df = df1.join(df2, (df1.text == df2.text), \"inner\").drop(df2[\"text\"])"
      ],
      "metadata": {
        "id": "xBL1CZSh9RoW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluator.evaluate(merge_df)\n",
        "precision = evaluator.evaluate(merge_df, {evaluator.metricName: \"precisionByLabel\"})\n",
        "recall = evaluator.evaluate(merge_df, {evaluator.metricName: \"recallByLabel\"})\n",
        "f1 = evaluator.evaluate(merge_df, {evaluator.metricName: \"f1\"})\n",
        "\n",
        "metrics_df = spark.createDataFrame([\n",
        "    Row(metric=\"\\nAccuracy\", score=accuracy),\n",
        "    Row(metric=\"\\nPrecision\", score=precision),\n",
        "    Row(metric=\"\\nRecall\", score=recall),\n",
        "    Row(metric=\"\\nF1 Score\", score=f1)\n",
        "])"
      ],
      "metadata": {
        "id": "wzG8UYZX9UKI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Metrics for Pretrained Pipline\\nIMDB Reviews (w/o Glove)\")\n",
        "metrics_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md-RlTg99W3l",
        "outputId": "b7d205ef-dca7-4ffa-ab1a-68d1ede201c9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics for Pretrained Pipline\n",
            "IMDB Reviews (w/o Glove)\n",
            "+-----------+--------------------+\n",
            "|     metric|               score|\n",
            "+-----------+--------------------+\n",
            "| \\nAccuracy|0.033460431094856434|\n",
            "|\\nPrecision|0.029610531661393023|\n",
            "|   \\nRecall|  0.9986824769433466|\n",
            "| \\nF1 Score|0.009602969181568641|\n",
            "+-----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import (\n",
        "    SentenceDetector,\n",
        "    Tokenizer,\n",
        "    WordEmbeddingsModel,\n",
        "    SentenceEmbeddings\n",
        ")\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "print(\"\\ndocumentAssembler finished!\")\n",
        "\n",
        "sentencer = SentenceDetector() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence\")\n",
        "print(\"\\nsentencer finished!\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "            .setInputCols([\"sentence\"]) \\\n",
        "            .setOutputCol(\"tokens\")\n",
        "print(\"\\ntokenizer finished!\")\n",
        "\n",
        "use = WordEmbeddingsModel.pretrained(\"glove_100d\")\\\n",
        "    .setInputCols(['document','tokens'])\\\n",
        "    .setOutputCol('word_embeddings')\n",
        "print(\"\\nWordEmbeddingsModel finished!\")\n",
        "\n",
        "sentence_embeddings = SentenceEmbeddings() \\\n",
        "    .setInputCols([\"document\", \"word_embeddings\"]) \\\n",
        "    .setOutputCol(\"sentence_embeddings\") \\\n",
        "    .setPoolingStrategy(\"AVERAGE\")\n",
        "print(\"\\nsentence_embeddings finished!\")\n",
        "\n",
        "sentimentdl = SentimentDLModel.pretrained(\"sentimentdl_glove_imdb\", \"en\")\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"prediction\")\n",
        "print(\"\\nsentimentdl finished!\")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "      stages = [\n",
        "          documentAssembler,\n",
        "          sentencer,\n",
        "          tokenizer,\n",
        "          use,\n",
        "          sentence_embeddings,\n",
        "          sentimentdl\n",
        "      ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bEbl1ua6x-f",
        "outputId": "7f87efea-365d-4578-85ac-1537bfb5d951"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "documentAssembler finished!\n",
            "\n",
            "sentencer finished!\n",
            "\n",
            "tokenizer finished!\n",
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n",
            "\n",
            "WordEmbeddingsModel finished!\n",
            "\n",
            "sentence_embeddings finished!\n",
            "sentimentdl_glove_imdb download started this may take some time.\n",
            "Approximate size to download 8.7 MB\n",
            "[OK!]\n",
            "\n",
            "sentimentdl finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the timer\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "print(\"\\nFitting model...\")\n",
        "\n",
        "start_time = time.time()\n",
        "pipelineModel = pipeline.fit(empty_df)\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"\\nModel fitted.\")\n",
        "\n",
        "# Calculate the total time taken\n",
        "total_time = end_time - start_time\n",
        "print(f\"\\nTotal execution time: {total_time} seconds\")\n",
        "\n",
        "# Start the timer\n",
        "print(\"\\nTesting model...\")\n",
        "start_time = time.time()\n",
        "\n",
        "preds = pipelineModel.transform(testDataset)\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"\\nTesting finished.\")\n",
        "\n",
        "# Calculate the total time taken\n",
        "total_time = end_time - start_time\n",
        "print(f\"\\nTotal execution time: {total_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyis5t4U67NB",
        "outputId": "5f7fcd22-63e8-42a2-b32b-b165e324fb26"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fitting model...\n",
            "\n",
            "Model fitted.\n",
            "\n",
            "Total execution time: 0.09493184089660645 seconds\n",
            "\n",
            "Testing model...\n",
            "\n",
            "Testing finished.\n",
            "\n",
            "Total execution time: 0.24004030227661133 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = preds.select(\"label\", \"text\")\n",
        "df2 = preds.select(\n",
        "    F.explode(\n",
        "      F.arrays_zip(\n",
        "        preds.document.result,\n",
        "        preds.prediction.result)).alias(\"cols\")\n",
        ").select(\n",
        "    F.expr(\"cols['0']\").alias(\"text\"),\n",
        "    F.expr(\"cols['1']\").alias(\"prediction\")\n",
        ")\n",
        "df2 = df2.withColumn(\n",
        "    \"prediction\",\n",
        "    F.when((df2[\"prediction\"] == \"positive\") | (df2[\"prediction\"] == \"neutral\"), 1.0).otherwise(0.0)\n",
        ")\n",
        "merge_df = df1.join(df2, (df1.text == df2.text), \"inner\").drop(df2[\"text\"])"
      ],
      "metadata": {
        "id": "vg8QHLKn7Fng"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluator.evaluate(merge_df)\n",
        "precision = evaluator.evaluate(merge_df, {evaluator.metricName: \"precisionByLabel\"})\n",
        "recall = evaluator.evaluate(merge_df, {evaluator.metricName: \"recallByLabel\"})\n",
        "f1 = evaluator.evaluate(merge_df, {evaluator.metricName: \"f1\"})\n",
        "\n",
        "metrics_df = spark.createDataFrame([\n",
        "    Row(metric=\"Accuracy\", score=accuracy),\n",
        "    Row(metric=\"Precision\", score=precision),\n",
        "    Row(metric=\"Recall\", score=recall),\n",
        "    Row(metric=\"F1 Score\", score=f1)\n",
        "])"
      ],
      "metadata": {
        "id": "XlLbXoS08Ud4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Metrics for Pretrained Pipline\\nIMDB Reviews (w/ Glove)\")\n",
        "metrics_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLnTWY4u8R5o",
        "outputId": "0370beab-3c08-415f-c4a9-a530c151abf2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics for Pretrained Pipline\n",
            "IMDB Reviews (w/ Glove)\n",
            "+---------+--------------------+\n",
            "|   metric|               score|\n",
            "+---------+--------------------+\n",
            "| Accuracy|  0.0496459419500428|\n",
            "|Precision|0.030136986301369864|\n",
            "|   Recall|                 1.0|\n",
            "| F1 Score| 0.04114126508468206|\n",
            "+---------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "T5TVRSzuuhwo"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}